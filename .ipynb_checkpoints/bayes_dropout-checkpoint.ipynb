{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lasagne imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import norm\n",
    "\n",
    "import lasagne\n",
    "import lasagne.layers.dnn\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.load_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            \n",
    "        ts = targets[excerpt]\n",
    "        ohe_labels = OneHotEncoder(10, sparse=False).fit_transform(ts[:,None])\n",
    "        yield inputs[excerpt], targets[excerpt]#ohe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#repeat batches for monte carlo sampling\n",
    "class RepeatLayer(lasagne.layers.Layer):\n",
    "    #tiles tensor in a given dimension\n",
    "    def __init__(self, incoming, reps, axis=0, **kwargs):\n",
    "        super(RepeatLayer, self).__init__(incoming, **kwargs)\n",
    "        self.reps = reps\n",
    "        self.axis = axis\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return tuple([None] + list(input_shape[1:]))\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return T.extra_ops.repeat(input, self.reps, axis=self.axis)\n",
    "    \n",
    "#obtain mean and var estimates from dropout samples\n",
    "class MCMomentsLayer(lasagne.layers.Layer):\n",
    "    #computes mean and variance from samples\n",
    "    def __init__(self, incoming, samples, **kwargs):\n",
    "        super(MCMomentsLayer, self).__init__(incoming, **kwargs)\n",
    "        self.samples = samples\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        #returns (mean, var)\n",
    "        return (None, 2)\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        #reshape input (1dim) so that congruent samples are rows\n",
    "        reshaped = T.reshape(input, (-1, self.samples))\n",
    "        samp_mean = T.mean(reshaped, axis=1)\n",
    "        samp_var = T.var(reshaped, axis=1)\n",
    "        return T.concatenate([samp_mean[:, None], samp_var[:, None]], axis=1) #samp_mean[:, None]#\n",
    "\n",
    "#build mlp with dropout, and monte carlo sampling\n",
    "def mc_regression(X, p=0.5, samples=20):\n",
    "    net = OrderedDict()\n",
    "\n",
    "    net['input'] = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), input_var=X)\n",
    "    net['repeat'] = RepeatLayer(net['input'], samples)\n",
    "    \n",
    "    #----\n",
    "    net['dense1'] = lasagne.layers.DenseLayer(net['repeat'], 1000, \\\n",
    "                                                   nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    net['drop1'] = lasagne.layers.DropoutLayer(net['dense1'], p=p)\n",
    "\n",
    "    #----\n",
    "    net['dense2'] = lasagne.layers.DenseLayer(net['drop1'], 1000, \\\n",
    "                                                   nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    net['drop2'] = lasagne.layers.DropoutLayer(net['dense2'], p=p)\n",
    "    \n",
    "    #----\n",
    "    net['dense3'] = lasagne.layers.DenseLayer(net['drop2'], 100, \\\n",
    "                                                   nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    net['drop3'] = lasagne.layers.DropoutLayer(net['dense3'], p=p)\n",
    "    \n",
    "    #----\n",
    "    net['softmax'] = lasagne.layers.DenseLayer(net['drop3'], 1, \\\n",
    "                                                   nonlinearity=lasagne.nonlinearities.linear)\n",
    "    \n",
    "    #----\n",
    "    net['bayes'] = MCMomentsLayer(net['softmax'], samples=samples)\n",
    "    \n",
    "    return net\n",
    "\n",
    "###--- explicit trueskill model\n",
    "#NB TS uncertainty might not make sense on MNIST since number is not a continuous visual property and the\n",
    "#GT comparisons here are generated on the true labels (rather than on some uncertainty in visual appearance)\n",
    "#i.e. the iffyness of the digit is not reflected in the comparison accuracy\n",
    "def normcdf(x, loc=0, scale=1):\n",
    "    #takes in vector - computes normcdf for each entry\n",
    "    return 0.5 * T.erfc(-(x) / T.sqrt(2))\n",
    "    \n",
    "class TSLayer(lasagne.layers.Layer):\n",
    "    #takes a four column input - (mu1, sigma1, mu2, sigma2)\n",
    "    \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        #calculate the probs of pairwise comparison win\n",
    "        \n",
    "        #NB might want to scale sigma values here (e.g. *10) to allow freedom for big values\n",
    "        \n",
    "        #set BETA var, i.e.where rating(mu+beta) > rating(mu) 76% of the time (with sigma1=sigma2=0)\n",
    "        #based on P(1>2) = normcdf[ (mu1-mu2) / (2*BETA^2) ] = 0.76\n",
    "        #in default, mu=25, sigma=8.33, beta=4.16, i.e. sigma = mu/3, beta=mu/6\n",
    "        #since mu here constrained to [0,1]:\n",
    "        #assume mu = 0.5 (mean rating) -> sigma = mu/3, beta = mu/6 = .0833\n",
    "        BETA = 0.08333\n",
    "        \n",
    "        #fraction calc for _normcdf\n",
    "        delta_mu = input[:,0] - input[:,2]\n",
    "        denom_s = (T.pow(input[:,1], 2) + T.pow(input[:,3], 2)) + (2 * BETA**2)\n",
    "        denom = T.sqrt(denom_s)\n",
    "        normcdf_arg = delta_mu/denom\n",
    "        \n",
    "        #calc the probs\n",
    "        p1 = 0.5 * T.erfc(-(normcdf_arg) / T.sqrt(2)) #normcdf calc\n",
    "        p2 = 0.5 * T.erfc(-(-normcdf_arg) / T.sqrt(2))\n",
    "        \n",
    "        return T.concatenate([p1[:,None], p2[:,None]], axis=1)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        batch_size = input_shape[0]\n",
    "        pred = input_shape[1]/2 #i.e. (mu1, sig1, mu2, sig2) => (P(1>2), P(2>1))        \n",
    "        return (batch_size, pred)\n",
    "\n",
    "def add_ts(net):\n",
    "    #puts truskill comparison layer on MCMomentEstimate layer\n",
    "    #input is a N*2 X 2 matrix, where N is number of matches\n",
    "    layer_list = [net[layer] for layer in net]\n",
    "    \n",
    "    #combine across 0th dimension \n",
    "    net['fusion'] = lasagne.layers.ReshapeLayer(layer_list[-1], (-1, 4)) #now has 4 columns (mu1, sigma1, mu2, sigma2)\n",
    "\n",
    "    #calc the ts probs\n",
    "    net['out'] = TSLayer(net['fusion'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CREATE NET\n",
    "#initialise inputs and outputs\n",
    "X = T.tensor4('X')\n",
    "y = T.ivector('y')\n",
    "\n",
    "#build net\n",
    "mc_samples = 40\n",
    "dropout_prob = 0.5\n",
    "net = mc_regression(X, p=dropout_prob, samples=mc_samples)\n",
    "\n",
    "#get output\n",
    "layer_list = [net[layer] for layer in net]\n",
    "full_output = lasagne.layers.get_output(layer_list)\n",
    "y_pred = full_output[-1][:,0] #just the point estimate for now\n",
    "\n",
    "#define regression loss\n",
    "loss = lasagne.objectives.squared_error(y_pred, y)\n",
    "loss = loss.mean()\n",
    "\n",
    "#define updates\n",
    "params = lasagne.layers.get_all_params(layer_list[-1], trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=.01, momentum=0.9)\n",
    "\n",
    "train_fn = theano.function(inputs = [X, y], outputs=loss, updates=updates, allow_input_downcast=True)\n",
    "check_fn = theano.function(inputs=[X], outputs=full_output, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mnist regression\n",
    "num_epochs = 100\n",
    "epoch_loss_hist = []\n",
    "for ep in xrange(num_epochs):\n",
    "    train_batches = 0\n",
    "    train_loss = 0\n",
    "    for Xb, yb in iterate_minibatches(X_train, y_train, 500):\n",
    "        train_loss += train_fn(Xb, yb) # <- trains the network\n",
    "        train_batches += 1\n",
    "        clear_output()\n",
    "        print train_batches\n",
    "        print 'epoch {0} results: train loss of {1}, val acc of na'.format(ep, \n",
    "                                                                         train_loss/float(train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = check_fn(X_val[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 5\n",
    "f, ax = plt.subplots(rows,cols, figsize=(16, 10))\n",
    "start_im=185\n",
    "for irow in xrange(rows):\n",
    "    for icol in xrange(cols):\n",
    "        ax[irow, icol].imshow(out[0][start_im,0], cmap='gray')\n",
    "        title_string = 'im: {2}, $\\mu = ${0:.2f}, $\\sigma = ${1:.2f}'.\\\n",
    "            format(out[-1][start_im, 0], pow(out[-1][start_im, 1],0.5), start_im)\n",
    "        ax[irow, icol].set_title(title_string)\n",
    "        start_im += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.erfc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
